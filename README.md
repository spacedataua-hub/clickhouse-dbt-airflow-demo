# clickhouse-dbt-airflow-demo

A demonstration project for showcasing **Data Engineering skills**.  
This repository integrates **ClickHouse**, **dbt**, and **Airflow** into a reproducible analytics pipeline, designed as a portfolio project.

## ğŸ¯ Purpose
- Clear example of modern data pipeline architecture  
- Orchestration with Airflow  
- Transformations with dbt  
- Analytics-ready modeling in ClickHouse  
- Reproducible workflows using Docker Compose and CI/CD  
- Monitoring, alerting, and healthcheck practices in Airflow  

## ğŸ› ï¸ Tech Stack
- ClickHouse â€” high-performance analytical database  
- dbt â€” SQL-based data transformations and modeling  
- Airflow â€” workflow orchestration, scheduling, monitoring, and alerting  
- Docker Compose â€” containerized local environment  
- Slack / Email Alerts â€” notifications on DAG failures  

## ğŸ“¦ Dependencies
- `airflow/requirements.txt` â€” Python packages for Airflow (providers, monitoring, utils)  
- `dbt/requirements.txt` â€” Python packages for dbt (dbt-core, dbt-clickhouse, dbt-utils)  

## ğŸ³ Custom Dockerfiles
- `airflow/Dockerfile` â€” builds Airflow image with dependencies and DAGs  
- `dbt/Dockerfile` â€” builds dbt image with dependencies and macros  

## ğŸ“‚ Project Structure
- `airflow/dags/affise_dag.py` â€” Main ETL DAG (Affise â†’ ClickHouse)  
- `airflow/dags/affise_healthcheck.py` â€” Healthcheck DAG for Affise API  
- `airflow/dags/dbt_model_run_daily.py` â€” DAG that runs the dbt model daily at 07:01, after successful completion of the Affise connector DAG. Includes Slack notifications and dbt testing.  
- `airflow/plugins/affise_connector.py` â€” Connector logic (API + ClickHouse)  
- `airflow/config/airflow.cfg` â€” Minimal Airflow configuration  
- `airflow/requirements.txt` â€” Airflow dependencies  
- `airflow/Dockerfile` â€” Custom Airflow image  
- `dbt/requirements.txt` â€” dbt dependencies  
- `dbt/Dockerfile` â€” Custom dbt image  
- `dbt/macros/convert_to_utc.sql` â€” Example macro for converting timestamps to UTC  
- `.env` â€” Environment variables (API keys, DB creds, Slack webhook)  
- `docker-compose.yml` â€” Local orchestration  
- `README.md` â€” Project documentation  

## ğŸš€ Airflow DAGs
- **affise_clickhouse_daily** â€” Runs daily at 07:00, fetches conversions from Affise API and stores them in ClickHouse. Includes retries, SLA, email alerts, and Slack notifications.  
- **affise_healthcheck** â€” Runs daily at 06:00, checks Affise API availability before the main ETL DAG. Alerts if API is unreachable.  
- **dbt_model_run_daily** â€” Runs daily at 07:01, executes the dbt model `final_model` with all dependencies resolved automatically.  
  - Waits for the completion of the `affise_clickhouse_daily` DAG (via `ExternalTaskSensor`).  
  - Runs `dbt test` after the model execution to validate data quality.  
  - Sends Slack and Email alerts if any task fails.  

## ğŸ“Š Monitoring & Alerts
- Email alerts â€” via `default_args` (`email_on_failure=True`)  
- Slack alerts â€” via `on_failure_callback` and webhook integration (`SLACK_WEBHOOK_URL` in `.env`)  
- Logs â€” stored locally, can be extended to ClickHouse or cloud storage  
- **SLA** â€” critical tasks can be configured with execution time limits (e.g., 30 minutes for the dbt DAG).  

## ğŸš€ Getting Started
1. Clone the repository:  
   ```bash
   git clone https://github.com/spacedataua-hub/clickhouse-dbt-airflow-demo.git
   cd clickhouse-dbt-airflow-demo